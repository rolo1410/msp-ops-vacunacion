[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Engine",
        "importPath": "sqlalchemy.engine",
        "description": "sqlalchemy.engine",
        "isExtraImport": true,
        "detail": "sqlalchemy.engine",
        "documentation": {}
    },
    {
        "label": "polars",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "polars",
        "description": "polars",
        "detail": "polars",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "polars",
        "description": "polars",
        "isExtraImport": true,
        "detail": "polars",
        "documentation": {}
    },
    {
        "label": "DB_VACUNACION",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_oracle_engine",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "duckdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "duckdb",
        "description": "duckdb",
        "detail": "duckdb",
        "documentation": {}
    },
    {
        "label": "persona_orchester",
        "importPath": "process.dim_persona",
        "description": "process.dim_persona",
        "isExtraImport": true,
        "detail": "process.dim_persona",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "process_data",
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "isExtraImport": true,
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "generare_bi_echema",
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "isExtraImport": true,
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "get_oracle_engine",
        "kind": 2,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "def get_oracle_engine(options: dict) -> Engine:\n    user = options.get(\"user\", \"user\")\n    password = options.get(\"password\", \"password\")\n    host = options.get(\"host\", \"localhost\")\n    port = options.get(\"port\", 1521)\n    service_name = options.get(\"service_name\", \"orclpdb1\")\n    connection_string = f'oracle+cx_oracle://{user}:{password}@{host}:{port}/{service_name}'\n    engine: Engine = create_engine(connection_string, pool_pre_ping=True)\n    return engine\ndef postgres_get_engine(options: dict) -> Engine:",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "postgres_get_engine",
        "kind": 2,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "def postgres_get_engine(options: dict) -> Engine:\n    user = options.get(\"user\", \"user\")\n    password = options.get(\"password\", \"password\")\n    host = options.get(\"host\", \"localhost\")\n    port = options.get(\"port\", 5432)\n    dbname = options.get(\"dbname\", \"dbname\")\n    connection_string = f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}'\n    engine: Engine = create_engine(connection_string, pool_pre_ping=True)\n    return engine",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_VACUNACION",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_VACUNACION = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_VACUNACION_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_SID\", \"orcl\")\n}\nDB_MIP = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_MPI_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_MPI_PORT\", 1521)),",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_MIP",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_MIP = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_MPI_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_MPI_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_MPI_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_MPI_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_MPI_SID\", \"orcl\")\n}\nDB_GEOSALUD = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_GEOSALUD_PORT\", 1521)),",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_GEOSALUD",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_GEOSALUD = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_GEOSALUD_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_DBNAME\", \"geoserver\")\n}\nDB_REPLICA = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_REPLICACION_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_REPLICACION_PORT\", 1521)),",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_REPLICA",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_REPLICA = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_REPLICACION_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_REPLICACION_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_REPLICACION_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_REPLICACION_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_REPLICACION_DBNAME\", \"replicacion\")\n}\ndef get_oracle_engine(options: dict) -> Engine:\n    user = options.get(\"user\", \"user\")\n    password = options.get(\"password\", \"password\")",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_db_vacunacion",
        "kind": 2,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "def get_db_vacunacion(since, until, offset=0, chunk_size=100000):\n    db_vacunacion_engine = get_oracle_engine(DB_VACUNACION)\n    query = \"\"\"\n            SELECT \n                ID_VAC_DEPU,\n                FECHA_APLICACION,\n                PUNTO_VACUNACION,\n                UNICODIGO,\n                APELLIDOS,\n                TIPO_IDEN,",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_count_db_vacunacion",
        "kind": 2,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "def get_count_db_vacunacion(since, until):\n    db_vacunacion_engine = get_oracle_engine(DB_VACUNACION)\n    query = \"\"\"\n            SELECT \n                COUNT(*) AS total_count\n            FROM HCUE_VACUNACION_DEPURADA.DB_VACUNACION_CONSOLIDADA_DEPURADA_COVID\n            WHERE \n                FECHA_APLICACION BETWEEN TO_DATE(:since, 'YYYY-MM-DD') \n                AND TO_DATE(:until, 'YYYY-MM-DD')\n            \"\"\"  # Replace with actual query",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_db_vacunaciones",
        "kind": 2,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "def get_db_vacunaciones(since, until):\n    offset = 0\n    chunk_size = 100000\n    all_data = []\n    while True:\n        chunk = get_db_vacunacion(since, until, offset, chunk_size)\n        if chunk.is_empty():\n            break\n        all_data.append(chunk)\n        offset += chunk_size",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "create_oracle_engine",
        "kind": 2,
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "peekOfCode": "def create_oracle_engine(config, type):\n    try:\n        engine = create_engine(\"postgresql+psycopg2://scott:tiger@localhost:5432/mydatabase\")\n        return engine\n    except Exception as e:\n        logging.info(f\"Error al establecer la conexión con la base de datos: {e}\")\ndef save_to_lake(data_chunk):\n    # Implement the logic to save the data chunk to the lake\n    logging.info(f\"Saving chunk of size {len(data_chunk)} to the lake...\")\n    pass    ",
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "save_to_lake",
        "kind": 2,
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "peekOfCode": "def save_to_lake(data_chunk):\n    # Implement the logic to save the data chunk to the lake\n    logging.info(f\"Saving chunk of size {len(data_chunk)} to the lake...\")\n    pass    \ndef process_data(engine, since, until, chunk_size=10000):\n    try:\n        with engine.connect() as connection:\n            result = connection.execute(\n                \"SELECT * FROM my_table WHERE date >= :since AND date < :until\",\n                {\"since\": since, \"until\": until}",
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "process_data",
        "kind": 2,
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "peekOfCode": "def process_data(engine, since, until, chunk_size=10000):\n    try:\n        with engine.connect() as connection:\n            result = connection.execute(\n                \"SELECT * FROM my_table WHERE date >= :since AND date < :until\",\n                {\"since\": since, \"until\": until}\n            )\n            for chunk in iter(lambda: list(itertools.islice(result, chunk_size)), []):\n                process_chunk(chunk)\n    except Exception as e:",
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "generate_lake_schema",
        "kind": 2,
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "peekOfCode": "def generate_lake_schema():\n    # Implement the logic to generate the lake schema\n    con = duckdb.connect('./resources/data_lake/vacunacion.duckdb')\n    con.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS lake_schema (\n            id INTEGER PRIMARY KEY,\n            name VARCHAR,\n            value DOUBLE\n        )\n    \"\"\")",
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "generare_bi_echema",
        "kind": 2,
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "peekOfCode": "def generare_bi_echema():\n    # Implement the logic to generate the BI schema\n    con = duckdb.connect('./resources/data_lake/vacunacion.duckdb')\n    con.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS dim_persona (\n            id INTEGER PRIMARY KEY,\n            nombres VARCHAR),\n            apellidos VARCHAR,\n            fecha_nacimiento DATE,\n            identificacion VARCHAR,",
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "lake.load_lake",
        "description": "lake.load_lake",
        "peekOfCode": "def load_data(df):\n    # Implement the logic to load data into the lake\n    logging.info(\"Loading data into the lake... postgres oracle\")   \n    pass",
        "detail": "lake.load_lake",
        "documentation": {}
    },
    {
        "label": "DATA_SOURCES",
        "kind": 5,
        "importPath": "lake.sources",
        "description": "lake.sources",
        "peekOfCode": "DATA_SOURCES = [\n    {\n        \"name\": \"MPI\",\n        \"type\": \"postgresql\",\n        \"host\": \"localhost\",\n        \"port\": 5432,\n        \"database\": \"mydatabase\",\n        \"username\": \"scott\",\n        \"password\": \"tiger\"              \n    }, {",
        "detail": "lake.sources",
        "documentation": {}
    },
    {
        "label": "limpiar_columas_texto",
        "kind": 2,
        "importPath": "process.dim_persona",
        "description": "process.dim_persona",
        "peekOfCode": "def limpiar_columas_texto(df: DataFrame):\n    logging.info(\"|- EST \")\n    logging.debug(\". |- No implementado\")\n    return df \ndef limpiar_columnas_fecha(df: DataFrame, cols: list):\n    logging.info(\"|- EST \")\n    logging.debug(\". |- No implementado\")\n    return df\ndef limpiar_identificacion(df: DataFrame):\n    logging.info(\"|- EST Estandarizando columnas cédulas\")",
        "detail": "process.dim_persona",
        "documentation": {}
    },
    {
        "label": "limpiar_columnas_fecha",
        "kind": 2,
        "importPath": "process.dim_persona",
        "description": "process.dim_persona",
        "peekOfCode": "def limpiar_columnas_fecha(df: DataFrame, cols: list):\n    logging.info(\"|- EST \")\n    logging.debug(\". |- No implementado\")\n    return df\ndef limpiar_identificacion(df: DataFrame):\n    logging.info(\"|- EST Estandarizando columnas cédulas\")\n    logging.debug(\". |- No implementadoEstandarizando columnas cédulas\")\n    return df\ndef validar_cedulas(df: DataFrame):\n    logging.info(\"|- VAL Estandarizando columnas cédulas\")",
        "detail": "process.dim_persona",
        "documentation": {}
    },
    {
        "label": "limpiar_identificacion",
        "kind": 2,
        "importPath": "process.dim_persona",
        "description": "process.dim_persona",
        "peekOfCode": "def limpiar_identificacion(df: DataFrame):\n    logging.info(\"|- EST Estandarizando columnas cédulas\")\n    logging.debug(\". |- No implementadoEstandarizando columnas cédulas\")\n    return df\ndef validar_cedulas(df: DataFrame):\n    logging.info(\"|- VAL Estandarizando columnas cédulas\")\n    logging.debug(\". |- No implementadoEstandarizando columnas cédulas\")\n    return df\ndef clean_anios_1900(df: DataFrame):\n    logging.info(\"|- VAL Estandarizando columnas cédulas\")",
        "detail": "process.dim_persona",
        "documentation": {}
    },
    {
        "label": "validar_cedulas",
        "kind": 2,
        "importPath": "process.dim_persona",
        "description": "process.dim_persona",
        "peekOfCode": "def validar_cedulas(df: DataFrame):\n    logging.info(\"|- VAL Estandarizando columnas cédulas\")\n    logging.debug(\". |- No implementadoEstandarizando columnas cédulas\")\n    return df\ndef clean_anios_1900(df: DataFrame):\n    logging.info(\"|- VAL Estandarizando columnas cédulas\")\n    logging.debug(\". |- No implementadoEstandarizando columnas cédulas\")\n    return df\ndef calcular_edad(df: DataFrame):\n    logging.info(\"|- ENR Agregando edad\")",
        "detail": "process.dim_persona",
        "documentation": {}
    },
    {
        "label": "clean_anios_1900",
        "kind": 2,
        "importPath": "process.dim_persona",
        "description": "process.dim_persona",
        "peekOfCode": "def clean_anios_1900(df: DataFrame):\n    logging.info(\"|- VAL Estandarizando columnas cédulas\")\n    logging.debug(\". |- No implementadoEstandarizando columnas cédulas\")\n    return df\ndef calcular_edad(df: DataFrame):\n    logging.info(\"|- ENR Agregando edad\")\n    logging.debug(\". |- No implementadoAgregando edad\")\n    logging.info(\"|- ENR Desagregando edad en días , mes , año\")\n    logging.debug(\". |- No implementadoDesagregando edad en días , mes , año\")\n    return df",
        "detail": "process.dim_persona",
        "documentation": {}
    },
    {
        "label": "calcular_edad",
        "kind": 2,
        "importPath": "process.dim_persona",
        "description": "process.dim_persona",
        "peekOfCode": "def calcular_edad(df: DataFrame):\n    logging.info(\"|- ENR Agregando edad\")\n    logging.debug(\". |- No implementadoAgregando edad\")\n    logging.info(\"|- ENR Desagregando edad en días , mes , año\")\n    logging.debug(\". |- No implementadoDesagregando edad en días , mes , año\")\n    return df\ndef persona_orchester(df: DataFrame):\n    df = limpiar_columas_texto(df)\n    df = limpiar_columnas_fecha(df)\n    df = limpiar_identificacion(df)",
        "detail": "process.dim_persona",
        "documentation": {}
    },
    {
        "label": "persona_orchester",
        "kind": 2,
        "importPath": "process.dim_persona",
        "description": "process.dim_persona",
        "peekOfCode": "def persona_orchester(df: DataFrame):\n    df = limpiar_columas_texto(df)\n    df = limpiar_columnas_fecha(df)\n    df = limpiar_identificacion(df)\n    return df",
        "detail": "process.dim_persona",
        "documentation": {}
    },
    {
        "label": "vacuna_orchester",
        "kind": 2,
        "importPath": "process.dim_vacuna",
        "description": "process.dim_vacuna",
        "peekOfCode": "def vacuna_orchester(df):\n    return df",
        "detail": "process.dim_vacuna",
        "documentation": {}
    },
    {
        "label": "vacunacion_orchester",
        "kind": 2,
        "importPath": "process.dim_vacunacion",
        "description": "process.dim_vacunacion",
        "peekOfCode": "def vacunacion_orchester(df: polars.DataFrame):\n    return df",
        "detail": "process.dim_vacunacion",
        "documentation": {}
    },
    {
        "label": "process_orchester",
        "kind": 2,
        "importPath": "process.process",
        "description": "process.process",
        "peekOfCode": "def process_orchester(df: DataFrame):\n    persona_orchester(df)",
        "detail": "process.process",
        "documentation": {}
    },
    {
        "label": "date_transform",
        "kind": 2,
        "importPath": "utils.clean.date_transform",
        "description": "utils.clean.date_transform",
        "peekOfCode": "def date_transform(df: DataFrame, column:str, format:str)->DataFrame:\n    df = df.with_columns(\n        polars.col(column).str.strptime(polars.Date,\n                                        fmt=format).alias(column)\n    )\n    return df",
        "detail": "utils.clean.date_transform",
        "documentation": {}
    },
    {
        "label": "remove_extra_whitespaces",
        "kind": 2,
        "importPath": "utils.clean.text_transform",
        "description": "utils.clean.text_transform",
        "peekOfCode": "def remove_extra_whitespaces(df: polars.DataFrame,\n                             columns: list[str]) -> polars.DataFrame:\n    \"\"\"\n    Remove extra whitespaces from the text.\n    Args:\n        df (polars.DataFrame): The input DataFrame.\n    Returns:\n        polars.DataFrame: The cleaned DataFrame.\n    \"\"\"\n    return df.with_columns(",
        "detail": "utils.clean.text_transform",
        "documentation": {}
    }
]