[
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "imagehash",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imagehash",
        "description": "imagehash",
        "detail": "imagehash",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "dotenv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dotenv",
        "description": "dotenv",
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "polars",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "polars",
        "description": "polars",
        "detail": "polars",
        "documentation": {}
    },
    {
        "label": "sqlalchemy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Engine",
        "importPath": "sqlalchemy.engine",
        "description": "sqlalchemy.engine",
        "isExtraImport": true,
        "detail": "sqlalchemy.engine",
        "documentation": {}
    },
    {
        "label": "URL",
        "importPath": "sqlalchemy.engine",
        "description": "sqlalchemy.engine",
        "isExtraImport": true,
        "detail": "sqlalchemy.engine",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "queue",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "queue",
        "description": "queue",
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "DB_VACUNACION",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_oracle_engine",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_REPLICA",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_VACUNACION",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_oracle_engine",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_GEOSALUD",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_oracle_engine",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_MIP",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_oracle_engine",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "add_new_elements_to_lake",
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "isExtraImport": true,
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "add_new_elements_to_lake",
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "isExtraImport": true,
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "add_new_elements_to_lake",
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "isExtraImport": true,
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "add_new_elements_to_lake",
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "isExtraImport": true,
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "generare_bi_echema",
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "isExtraImport": true,
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "get_db_vacunaciones_cached",
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "isExtraImport": true,
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_db_vacunaciones_parallel",
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "isExtraImport": true,
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_db_vacunaciones_parallel_rutinario",
        "importPath": "extract.db_vacunacion_rutinario",
        "description": "extract.db_vacunacion_rutinario",
        "isExtraImport": true,
        "detail": "extract.db_vacunacion_rutinario",
        "documentation": {}
    },
    {
        "label": "get_geo_salud_data",
        "importPath": "extract.geo_salud",
        "description": "extract.geo_salud",
        "isExtraImport": true,
        "detail": "extract.geo_salud",
        "documentation": {}
    },
    {
        "label": "get_mpi_data",
        "importPath": "extract.mpi",
        "description": "extract.mpi",
        "isExtraImport": true,
        "detail": "extract.mpi",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "lake.load_lake",
        "description": "lake.load_lake",
        "isExtraImport": true,
        "detail": "lake.load_lake",
        "documentation": {}
    },
    {
        "label": "get_identificaciones_data",
        "importPath": "lake.load_lake",
        "description": "lake.load_lake",
        "isExtraImport": true,
        "detail": "lake.load_lake",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "lake.load_lake",
        "description": "lake.load_lake",
        "isExtraImport": true,
        "detail": "lake.load_lake",
        "documentation": {}
    },
    {
        "label": "duckdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "duckdb",
        "description": "duckdb",
        "detail": "duckdb",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "generate_profile_report",
        "importPath": "load.generate_profile",
        "description": "load.generate_profile",
        "isExtraImport": true,
        "detail": "load.generate_profile",
        "documentation": {}
    },
    {
        "label": "limpiar_columnas_geograficas",
        "importPath": "process.clean_transform.dim_establecimiento",
        "description": "process.clean_transform.dim_establecimiento",
        "isExtraImport": true,
        "detail": "process.clean_transform.dim_establecimiento",
        "documentation": {}
    },
    {
        "label": "persona_orchester",
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "isExtraImport": true,
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "vacuna_orchester",
        "importPath": "process.clean_transform.dim_vacuna",
        "description": "process.clean_transform.dim_vacuna",
        "isExtraImport": true,
        "detail": "process.clean_transform.dim_vacuna",
        "documentation": {}
    },
    {
        "label": "vacunacion_orchester",
        "importPath": "process.clean_transform.dim_vacunacion",
        "description": "process.clean_transform.dim_vacunacion",
        "isExtraImport": true,
        "detail": "process.clean_transform.dim_vacunacion",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ingest_orchester",
        "importPath": "extract.ingest_orchester",
        "description": "extract.ingest_orchester",
        "isExtraImport": true,
        "detail": "extract.ingest_orchester",
        "documentation": {}
    },
    {
        "label": "profiler_orchester",
        "importPath": "load.profilers.persona_profiler",
        "description": "load.profilers.persona_profiler",
        "isExtraImport": true,
        "detail": "load.profilers.persona_profiler",
        "documentation": {}
    },
    {
        "label": "process_orchester",
        "importPath": "process.clean_transform_orchester",
        "description": "process.clean_transform_orchester",
        "isExtraImport": true,
        "detail": "process.clean_transform_orchester",
        "documentation": {}
    },
    {
        "label": "find_similar_images",
        "kind": 2,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "def find_similar_images(userpaths, hashfunc=imagehash.average_hash):\n\tdef is_image(filename):\n\t\tf = filename.lower()\n\t\treturn f.endswith('.png') or f.endswith('.jpg') or \\\n\t\t\tf.endswith('.jpeg') or f.endswith('.bmp') or \\\n\t\t\tf.endswith('.gif') or '.jpg' in f or f.endswith('.svg')\n\timage_filenames = []\n\tfor userpath in userpaths:\n\t\timage_filenames += [os.path.join(userpath, path) for path in os.listdir(userpath) if is_image(path)]\n\timages = {}",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\tf",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\t\tf = filename.lower()\n\t\treturn f.endswith('.png') or f.endswith('.jpg') or \\\n\t\t\tf.endswith('.jpeg') or f.endswith('.bmp') or \\\n\t\t\tf.endswith('.gif') or '.jpg' in f or f.endswith('.svg')\n\timage_filenames = []\n\tfor userpath in userpaths:\n\t\timage_filenames += [os.path.join(userpath, path) for path in os.listdir(userpath) if is_image(path)]\n\timages = {}\n\tfor img in sorted(image_filenames):\n\t\ttry:",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\timage_filenames",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\timage_filenames = []\n\tfor userpath in userpaths:\n\t\timage_filenames += [os.path.join(userpath, path) for path in os.listdir(userpath) if is_image(path)]\n\timages = {}\n\tfor img in sorted(image_filenames):\n\t\ttry:\n\t\t\thash = hashfunc(Image.open(img))\n\t\texcept Exception as e:\n\t\t\tprint('Problem:', e, 'with', img)\n\t\t\tcontinue",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\timages",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\timages = {}\n\tfor img in sorted(image_filenames):\n\t\ttry:\n\t\t\thash = hashfunc(Image.open(img))\n\t\texcept Exception as e:\n\t\t\tprint('Problem:', e, 'with', img)\n\t\t\tcontinue\n\t\tif hash in images:\n\t\t\tprint(img, '  already exists as', ' '.join(images[hash]))\n\t\t\tif 'dupPictures' in img:",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\t\thash",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\t\t\thash = hashfunc(Image.open(img))\n\t\texcept Exception as e:\n\t\t\tprint('Problem:', e, 'with', img)\n\t\t\tcontinue\n\t\tif hash in images:\n\t\t\tprint(img, '  already exists as', ' '.join(images[hash]))\n\t\t\tif 'dupPictures' in img:\n\t\t\t\tprint('rm -v', img)\n\t\timages[hash] = images.get(hash, []) + [img]\n\t# for k, img_list in six.iteritems(images):",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\timages[hash]",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\t\timages[hash] = images.get(hash, []) + [img]\n\t# for k, img_list in six.iteritems(images):\n\t# \tif len(img_list) > 1:\n\t# \t\tprint(\" \".join(img_list))\nif __name__ == '__main__':  # noqa: C901\n\timport os\n\timport sys\n\tdef usage():\n\t\tsys.stderr.write(\"\"\"SYNOPSIS: %s [ahash|phash|dhash|...] [<directory>]\nIdentifies similar images in the directory.",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\thashmethod",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\thashmethod = sys.argv[1] if len(sys.argv) > 1 else usage()\n\tif hashmethod == 'ahash':\n\t\thashfunc = imagehash.average_hash\n\telif hashmethod == 'phash':\n\t\thashfunc = imagehash.phash\n\telif hashmethod == 'dhash':\n\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.average_hash\n\telif hashmethod == 'phash':\n\t\thashfunc = imagehash.phash\n\telif hashmethod == 'dhash':\n\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.phash\n\telif hashmethod == 'dhash':\n\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')\n\telif hashmethod == 'colorhash':\n\t\thashfunc = imagehash.colorhash",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')\n\telif hashmethod == 'colorhash':\n\t\thashfunc = imagehash.colorhash\n\telif hashmethod == 'crop-resistant':\n\t\thashfunc = imagehash.crop_resistant_hash",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')\n\telif hashmethod == 'colorhash':\n\t\thashfunc = imagehash.colorhash\n\telif hashmethod == 'crop-resistant':\n\t\thashfunc = imagehash.crop_resistant_hash\n\telse:\n\t\tusage()",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.colorhash\n\telif hashmethod == 'crop-resistant':\n\t\thashfunc = imagehash.crop_resistant_hash\n\telse:\n\t\tusage()\n\tuserpaths = sys.argv[2:] if len(sys.argv) > 2 else '.'\n\tfind_similar_images(userpaths=userpaths, hashfunc=hashfunc)",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.crop_resistant_hash\n\telse:\n\t\tusage()\n\tuserpaths = sys.argv[2:] if len(sys.argv) > 2 else '.'\n\tfind_similar_images(userpaths=userpaths, hashfunc=hashfunc)",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\tuserpaths",
        "kind": 5,
        "importPath": ".venv.bin.find_similar_images",
        "description": ".venv.bin.find_similar_images",
        "peekOfCode": "\tuserpaths = sys.argv[2:] if len(sys.argv) > 2 else '.'\n\tfind_similar_images(userpaths=userpaths, hashfunc=hashfunc)",
        "detail": ".venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "get_source_query",
        "kind": 2,
        "importPath": "extract.config.queries",
        "description": "extract.config.queries",
        "peekOfCode": "def get_source_query(source, tipe):\n    if source == 'vacunacion':\n        if tipe == 'data':\n            return SELECT_VACUNACION_COVID_19\n        elif tipe == 'count':\n            return COUNT_VACUNACION_COVID_19\n    else:\n        raise ValueError(f\"Fuente desconocida: {source}\")",
        "detail": "extract.config.queries",
        "documentation": {}
    },
    {
        "label": "get_oracle_engine",
        "kind": 2,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "def get_oracle_engine(options: dict) -> Engine:\n    user = options.get(\"user\", \"user\")\n    password = options.get(\"password\", \"password\")\n    host = options.get(\"host\", \"localhost\")\n    port = options.get(\"port\", 1521)\n    service_name = options.get(\"service_name\", \"orclpdb1\")\n    connection_string = f'oracle+oracledb://{user}:{password}@{host}:{port}/?service_name={service_name}'\n    engine: Engine = create_engine(connection_string, pool_pre_ping=True)\n    return engine\ndef postgres_get_engine(options: dict) -> Engine:",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "postgres_get_engine",
        "kind": 2,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "def postgres_get_engine(options: dict) -> Engine:\n    user = options.get(\"user\", \"user\")\n    password = options.get(\"password\", \"password\")\n    host = options.get(\"host\", \"localhost\")\n    port = options.get(\"port\", 5432)\n    dbname = options.get(\"dbname\", \"dbname\")\n    connection_string = f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}'\n    engine: Engine = create_engine(connection_string, pool_pre_ping=True)\n    return engine",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_VACUNACION",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_VACUNACION = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_VACUNACION_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_SID\", \"orcl\")\n}\nDB_MIP = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_MPI_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_MPI_PORT\", 1521)),",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_MIP",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_MIP = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_MPI_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_MPI_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_MPI_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_MPI_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_MPI_SID\", \"orcl\")\n}\nDB_GEOSALUD = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_GEOSALUD_PORT\", 1521)),",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_GEOSALUD",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_GEOSALUD = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_GEOSALUD_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_DB_NAME\", \"geoserver\")\n}\nDB_REPLICA = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_REPLICA_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_REPLICA_PORT\", 1521)),",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_REPLICA",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_REPLICA = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_REPLICA_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_REPLICA_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_REPLICA_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_REPLICA_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_REPLICA_DB_NAME\", \"reeplicas\")\n}\ndef get_oracle_engine(options: dict) -> Engine:\n    user = options.get(\"user\", \"user\")\n    password = options.get(\"password\", \"password\")",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_db_vacunacion_optimized",
        "kind": 2,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "def get_db_vacunacion_optimized(since: str, until: str, offset: int = 0, chunk_size: int = 100000) -> pl.DataFrame:\n    \"\"\"\n    Versión optimizada de la consulta con mejores prácticas SQL\n    \"\"\"\n    db_vacunacion_engine = get_oracle_engine(DB_VACUNACION)\n    # Query optimizada con hints de Oracle y ORDER BY para consistencia\n    query = f\"\"\"\n            SELECT /*+ FIRST_ROWS({chunk_size}) INDEX_RANGE_SCAN */\n                ID_VAC_DEPU,\n                FECHA_APLICACION,",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_db_vacunacion",
        "kind": 2,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "def get_db_vacunacion(since, until, offset=0, chunk_size=100000) -> pl.DataFrame:\n    return get_db_vacunacion_optimized(since, until, offset, chunk_size)\ndef get_count_db_vacunacion(since, until):\n    db_vacunacion_engine = get_oracle_engine(DB_VACUNACION)\n    query = f\"\"\"\n            SELECT \n                COUNT(*) AS total_count\n            FROM HCUE_VACUNACION_DEPURADA.DB_VACUNACION_CONSOLIDADA_DEPURADA_COVID\n            WHERE \n                FECHA_APLICACION BETWEEN TO_DATE('{since}', 'YYYY-MM-DD') ",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_count_db_vacunacion",
        "kind": 2,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "def get_count_db_vacunacion(since, until):\n    db_vacunacion_engine = get_oracle_engine(DB_VACUNACION)\n    query = f\"\"\"\n            SELECT \n                COUNT(*) AS total_count\n            FROM HCUE_VACUNACION_DEPURADA.DB_VACUNACION_CONSOLIDADA_DEPURADA_COVID\n            WHERE \n                FECHA_APLICACION BETWEEN TO_DATE('{since}', 'YYYY-MM-DD') \n                AND TO_DATE('{until}', 'YYYY-MM-DD')\n            \"\"\"  # Replace with actual query",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_db_vacunaciones_parallel",
        "kind": 2,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "def get_db_vacunaciones_parallel(since: str, until: str, chunk_size: int = 500000, max_workers: int = 4) -> None:\n    \"\"\"\n    Versión paralela optimizada para obtener datos de vacunación y persistirlos usando cola.\n    No retorna DataFrame, sino que persiste directamente cada chunk al lago de datos.\n    \"\"\"\n    logging.info(f\"|- Fetching vacunas (paralelo con {max_workers} workers)\")\n    start_total = time.time()\n    # Obtener total de registros\n    total = get_count_db_vacunacion(since, until)\n    logging.info(f\" |- Total de registros: {total:,}\")",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "VACUNACION_SCHEMA",
        "kind": 5,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "VACUNACION_SCHEMA = {\n    \"ID_VAC_DEPU\": pl.String,\n    \"FECHA_APLICACION\": pl.Date,\n    \"PUNTO_VACUNACION\": pl.String,\n    \"UNICODIGO\": pl.String,\n    \"TIPO_IDEN\": pl.String,\n    \"NUM_IDEN\": pl.String,\n    \"APELLIDOS\": pl.String,\n    \"NOMBRES\": pl.String,\n    \"NOMBRES_COMPLETOS\": pl.String,",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_db_vacunacion_rutinario_optimized",
        "kind": 2,
        "importPath": "extract.db_vacunacion_rutinario",
        "description": "extract.db_vacunacion_rutinario",
        "peekOfCode": "def get_db_vacunacion_rutinario_optimized(since: str, until: str, offset: int = 0, chunk_size: int = 100000) -> pl.DataFrame:\n    \"\"\"\n    Versión optimizada de la consulta con mejores prácticas SQL\n    \"\"\"\n    db_vacunacion_engine = get_oracle_engine(DB_REPLICA)\n    # Query optimizada con hints de Oracle y ORDER BY para consistencia\n    query = f\"\"\"\n            SELECT /*+ FIRST_ROWS({chunk_size}) INDEX_RANGE_SCAN */\n                R.ID,\n                'HACUE_AMED',",
        "detail": "extract.db_vacunacion_rutinario",
        "documentation": {}
    },
    {
        "label": "get_db_vacunacion",
        "kind": 2,
        "importPath": "extract.db_vacunacion_rutinario",
        "description": "extract.db_vacunacion_rutinario",
        "peekOfCode": "def get_db_vacunacion(since, until, offset=0, chunk_size=100000) -> pl.DataFrame:\n    return get_db_vacunacion_rutinario_optimized(since, until, offset, chunk_size)\ndef get_count_db_vacunacion_rutinario(since, until):\n    db_vacunacion_engine = get_oracle_engine(DB_REPLICA)\n    query = f\"\"\"\n            SELECT\n                count(*) as TOTAL_COUNT\n            FROM\n                HCUE_AMED.REGISTROVACUNACION R\n            WHERE",
        "detail": "extract.db_vacunacion_rutinario",
        "documentation": {}
    },
    {
        "label": "get_count_db_vacunacion_rutinario",
        "kind": 2,
        "importPath": "extract.db_vacunacion_rutinario",
        "description": "extract.db_vacunacion_rutinario",
        "peekOfCode": "def get_count_db_vacunacion_rutinario(since, until):\n    db_vacunacion_engine = get_oracle_engine(DB_REPLICA)\n    query = f\"\"\"\n            SELECT\n                count(*) as TOTAL_COUNT\n            FROM\n                HCUE_AMED.REGISTROVACUNACION R\n            WHERE\n                R.FECHAVACUNACION > TO_DATE('{since}', 'yyyy-mm-dd') \n                AND R.FECHAVACUNACION < TO_DATE('{until}', 'yyyy-mm-dd')",
        "detail": "extract.db_vacunacion_rutinario",
        "documentation": {}
    },
    {
        "label": "get_db_vacunaciones_parallel_rutinario",
        "kind": 2,
        "importPath": "extract.db_vacunacion_rutinario",
        "description": "extract.db_vacunacion_rutinario",
        "peekOfCode": "def get_db_vacunaciones_parallel_rutinario(since: str, until: str, chunk_size: int = 500000, max_workers: int = 4) -> None:\n    \"\"\"\n    Versión paralela optimizada para obtener datos de vacunación y persistirlos usando cola.\n    No retorna DataFrame, sino que persiste directamente cada chunk al lago de datos.\n    \"\"\"\n    logging.info(f\"|- Fetching vacunas (paralelo con {max_workers} workers)\")\n    start_total = time.time()\n    # Obtener total de registros\n    total = get_count_db_vacunacion_rutinario(since, until)\n    logging.info(f\" |- Total de registros: {total:,}\")",
        "detail": "extract.db_vacunacion_rutinario",
        "documentation": {}
    },
    {
        "label": "get_db_vacunaciones",
        "kind": 2,
        "importPath": "extract.db_vacunacion_rutinario",
        "description": "extract.db_vacunacion_rutinario",
        "peekOfCode": "def get_db_vacunaciones(since, until, chunk_size=500000) -> pl.DataFrame:\n    \"\"\"\n    Función principal optimizada - usa paralelización por defecto\n    \"\"\"\n    # Determinar número óptimo de workers basado en CPU\n    max_workers = min(4, os.cpu_count() or 1)\n    # Usar versión con cache si está disponible\n    try:\n        logging.warning(f\"Error con cache, usando versión paralela: {e}\")\n        # La función paralela ya no retorna DataFrame, persiste directamente",
        "detail": "extract.db_vacunacion_rutinario",
        "documentation": {}
    },
    {
        "label": "get_db_vacunaciones_from_lake",
        "kind": 2,
        "importPath": "extract.db_vacunacion_rutinario",
        "description": "extract.db_vacunacion_rutinario",
        "peekOfCode": "def get_db_vacunaciones_from_lake() -> pl.DataFrame:\n    \"\"\"\n    Función de utilidad para cargar datos de vacunación directamente desde el lago.\n    Útil cuando se necesita el DataFrame completo después de usar la versión paralela.\n    \"\"\"\n    from lake.load_lake import load_data\n    logging.info(\"|- Cargando datos de vacunación desde el lago\")\n    df = load_data()\n    logging.info(f\" |- Cargados {df.shape[0]:,} registros, {df.shape[1]} columnas\")\n    return df",
        "detail": "extract.db_vacunacion_rutinario",
        "documentation": {}
    },
    {
        "label": "VACUNACION_REGULAR_SCHEMA",
        "kind": 5,
        "importPath": "extract.db_vacunacion_rutinario",
        "description": "extract.db_vacunacion_rutinario",
        "peekOfCode": "VACUNACION_REGULAR_SCHEMA = {\n    \"ID\":pl.Int64,\n    'HACUE_AMED':pl.String,\n    \"ACTIVO\":pl.Int64, \n    \"PACIENTE_ID\":pl.Int64,\n    \"PERSONA_ID\":pl.Int64,\n    \"FECHAVACUNACION\" :pl.Date,\n    \"NUMEROIDENTIFICACION\" :pl.String,\n    \"FECHANACIMIENTO\":pl.Date,\n    \"ESTADO\":pl.Int64,",
        "detail": "extract.db_vacunacion_rutinario",
        "documentation": {}
    },
    {
        "label": "get_geo_salud_data",
        "kind": 2,
        "importPath": "extract.geo_salud",
        "description": "extract.geo_salud",
        "peekOfCode": "def get_geo_salud_data():\n    db_geo_salud_data = get_oracle_engine(DB_GEOSALUD)\n    query = f\"\"\"\n            SELECT\n                UNI_CODIGO,\n                UNI_NOMBRE,\n                PRV_CODIGO,\n                PRV_DESCRIPCION,\n                CAN_CODIGO,\n                CAN_DESCRIPCION,",
        "detail": "extract.geo_salud",
        "documentation": {}
    },
    {
        "label": "ingest_orchester",
        "kind": 2,
        "importPath": "extract.ingest_orchester",
        "description": "extract.ingest_orchester",
        "peekOfCode": "def ingest_orchester(since, until, chunk_size=500000, max_workers=4, use_cache=True):\n    \"\"\"\n    Orquestador de ingesta optimizado con parámetros configurables\n    \"\"\"\n    logging.info(\"|- Usando versión paralela con persistencia automática\")\n    # La función ya no retorna DataFrame, persiste directamente en una base de datos duckdb\n    # get_db_vacunaciones_parallel(since, until, chunk_size, max_workers)\n    ## obtiene los datos de vacunación de rutina\n    ##get_db_vacunaciones_parallel_rutinario(since, until, chunk_size, max_workers)\n    # Cargar datos desde el lago para obtener las identificaciones",
        "detail": "extract.ingest_orchester",
        "documentation": {}
    },
    {
        "label": "get_mpi_data_chunk",
        "kind": 2,
        "importPath": "extract.mpi",
        "description": "extract.mpi",
        "peekOfCode": "def get_mpi_data_chunk(identifications: list[str]) -> pl.DataFrame:\n    db_mpi_data = get_oracle_engine(DB_MIP)\n    query = f\"\"\"\n            SELECT\n                EC_IDENTIFIER_OID,\n                IDENTIFIER_VALUE,\n                GENDER,\n                BIRTHDATE,\n                MARITAL_STATUS,\n                EC_FAMILY_GROUP,",
        "detail": "extract.mpi",
        "documentation": {}
    },
    {
        "label": "get_mpi_data",
        "kind": 2,
        "importPath": "extract.mpi",
        "description": "extract.mpi",
        "peekOfCode": "def get_mpi_data(identifications: list[str]) -> pl.DataFrame:\n    logging.info(f\"|- MPI Obteniendo datos del MPI para {len(identifications)} identificaciones\")\n    chunk_size = 999\n    dfs = []\n    # por configuracion solo se puede traer en chunks de 999\n    for i in range(0, len(identifications), chunk_size):\n        logging.info(f\" |- Fetching chunk {i // chunk_size + 1} de {(len(identifications) - 1) // chunk_size + 1}\")\n        chunk = identifications[i:i + chunk_size]\n        df = get_mpi_data_chunk(chunk)\n        dfs.append(df)",
        "detail": "extract.mpi",
        "documentation": {}
    },
    {
        "label": "create_oracle_engine",
        "kind": 2,
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "peekOfCode": "def create_oracle_engine(config, type):\n    try:\n        engine = create_engine(\"postgresql+psycopg2://scott:tiger@localhost:5432/mydatabase\")\n        return engine\n    except Exception as e:\n        logging.info(f\"Error al establecer la conexión con la base de datos: {e}\")\ndef save_to_lake(data_chunk):\n    # Implement the logic to save the data chunk to the lake\n    logging.info(f\"Saving chunk of size {len(data_chunk)} to the lake...\")\n    pass    ",
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "save_to_lake",
        "kind": 2,
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "peekOfCode": "def save_to_lake(data_chunk):\n    # Implement the logic to save the data chunk to the lake\n    logging.info(f\"Saving chunk of size {len(data_chunk)} to the lake...\")\n    pass    \ndef process_data(engine, since, until, chunk_size=10000):\n    try:\n        with engine.connect() as connection:\n            result = connection.execute(\n                \"SELECT * FROM my_table WHERE date >= :since AND date < :until\",\n                {\"since\": since, \"until\": until}",
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "process_data",
        "kind": 2,
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "peekOfCode": "def process_data(engine, since, until, chunk_size=10000):\n    try:\n        with engine.connect() as connection:\n            result = connection.execute(\n                \"SELECT * FROM my_table WHERE date >= :since AND date < :until\",\n                {\"since\": since, \"until\": until}\n            )\n            for chunk in iter(lambda: list(itertools.islice(result, chunk_size)), []):\n                process_chunk(chunk)\n    except Exception as e:",
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "ingest_data_lake",
        "kind": 2,
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "peekOfCode": "def ingest_data_lake(since, until, chunk_size=10000):\n    logging.basicConfig(level=logging.INFO)\n    logging.info(\"Iniciando el proceso de ingestión de datos al lago...\")\n    engine = create_oracle_engine(\"DB_CONFIG\", \"oracle\")\n    if not engine:\n        logging.error(\"No se pudo crear el motor de la base de datos. Terminando el proceso.\")\n        return\n    process_data(engine, since, until, chunk_size)\n    logging.info(\"Proceso de ingestión de datos al lago completado.\")",
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "generate_lake_schema",
        "kind": 2,
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "peekOfCode": "def generate_lake_schema():\n    # Implement the logic to generate the lake schema\n    con = duckdb.connect('./resources/data_lake/vacunacion.duckdb')\n    con.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS lake_schema (\n            id INTEGER PRIMARY KEY,\n            name VARCHAR,\n            value DOUBLE\n        )\n    \"\"\")",
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "generate_bi_schema",
        "kind": 2,
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "peekOfCode": "def generate_bi_schema():\n    # Implement the logic to generate the BI schema\n    con = duckdb.connect('./resources/data_lake/vacunacion_schema.duckdb')\n    con.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS dim_persona (\n            id INTEGER PRIMARY KEY,\n            nombres VARCHAR,\n            apellidos VARCHAR,\n            fecha_nacimiento DATE,\n            identificacion VARCHAR,",
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "add_new_elements_to_lake",
        "kind": 2,
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "peekOfCode": "def add_new_elements_to_lake( db:str,\n                              table:str,\n                              keys_columns:list[str],\n                              df:pl.DataFrame):\n    logging.info(f\"|-Adding new elements to lake: {db}.{table}\")\n    # Implement the logic to add new elements to the lake\n    con = duckdb.connect(f'./resources/data_lake/{db}.duckdb')\n    # aqui hace el llamado al df\n    one_query = f\"\"\"CREATE TABLE IF NOT EXISTS {db}.main.{table} AS SELECT * FROM df;\n                    CREATE TABLE IF NOT EXISTS {db}.main.tmp_{table} AS SELECT * FROM df;",
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "lake.load_lake",
        "description": "lake.load_lake",
        "peekOfCode": "def load_data()-> pl.DataFrame:\n    # Implement the logic to load data into the lake\n    logging.info(\"|- Cargando datos al lago\")\n    con = duckdb.connect(database='./resources/data_lake/vacunacion.duckdb')\n    df = con.execute(\"SELECT * FROM vacunacion\").fetch_df()\n    df = pl.from_pandas(df)\n    logging.info(\" |- Datos cargados al lago\")\n    return df\ndef get_identificaciones_data()-> pl.DataFrame:\n    # Implement the logic to load data into the lake",
        "detail": "lake.load_lake",
        "documentation": {}
    },
    {
        "label": "get_identificaciones_data",
        "kind": 2,
        "importPath": "lake.load_lake",
        "description": "lake.load_lake",
        "peekOfCode": "def get_identificaciones_data()-> pl.DataFrame:\n    # Implement the logic to load data into the lake\n    logging.info(\"|- Cargando datos al lago\")\n    con = duckdb.connect(database='./resources/data_lake/vacunacion.duckdb')\n    df = con.execute(\"select distinct(v.num_iden) from vacunacion v \").fetch_df()\n    ## remover \" y ' de las identificaciones :\"\n    df['num_iden'] = df['num_iden'].str.replace(':','').str.replace('\"','').str.replace(\"'\",'')\n    df = pl.from_pandas(df)\n    logging.info(\" |- Datos cargados al lago\")\n    return df",
        "detail": "lake.load_lake",
        "documentation": {}
    },
    {
        "label": "DATA_SOURCES",
        "kind": 5,
        "importPath": "lake.sources",
        "description": "lake.sources",
        "peekOfCode": "DATA_SOURCES = [\n    {\n        \"name\": \"MPI\",\n        \"type\": \"postgresql\",\n        \"host\": \"localhost\",\n        \"port\": 5432,\n        \"database\": \"mydatabase\",\n        \"username\": \"scott\",\n        \"password\": \"tiger\"              \n    }, {",
        "detail": "lake.sources",
        "documentation": {}
    },
    {
        "label": "profiler_orchester",
        "kind": 2,
        "importPath": "load.profilers.persona_profiler",
        "description": "load.profilers.persona_profiler",
        "peekOfCode": "def profiler_orchester(df: pl.DataFrame):\n    if os.getenv('GENERATE_PERFILER', 'False') == 'True':\n        __generic_profiler(df,'vacuna', ['NOMBRE_VACUNA', 'LOTE_VACUNA'], 'vacuna')\n        __generic_profiler(df,'vacunacion', ['FECHA_APLICACION', 'UNICODIGO'], 'vacunacion')\n        __generic_profiler(df,'establecimiento', ['UNICODIGO', 'PUNTO_VACUNACION'], 'establecimiento')\n        __generic_profiler(df,'persona', ['NUM_IDEN', 'FECHA_APLICACION', 'UNICODIGO'], 'persona')\n    return df",
        "detail": "load.profilers.persona_profiler",
        "documentation": {}
    },
    {
        "label": "generate_profile_report",
        "kind": 2,
        "importPath": "load.generate_profile",
        "description": "load.generate_profile",
        "peekOfCode": "def generate_profile_report(df: pl.DataFrame, columns: list[str],path: str, name: str):\n    \"\"\"\n    Genera un perfil de datos usando ydata_profiling si está disponible.\n    Si no está disponible, genera un reporte básico.\n    \"\"\"\n    logging.info(\"|- ENR Generando perfil de datos\")\n    try:\n        from ydata_profiling import ProfileReport\n        logging.debug(\" |- Usando ydata_profiling para generar perfil completo\")\n        # Seleccionar columnas y convertir a pandas para ydata_profiling",
        "detail": "load.generate_profile",
        "documentation": {}
    },
    {
        "label": "limpiar_columnas_geograficas",
        "kind": 2,
        "importPath": "process.clean_transform.dim_establecimiento",
        "description": "process.clean_transform.dim_establecimiento",
        "peekOfCode": "def limpiar_columnas_geograficas(df: pl.DataFrame, cols: list):\n    logging.info(\"|- EST \")\n    logging.debug(\" |- Truncando latitud y longitud a 6 decimales\")\n    df = df.with_columns(\n        [pl.col(col).round(6) for col in cols if col in df.columns]\n    )  \n    return df\ndef limpiar_columas_texto(df: pl.DataFrame):\n    logging.info(\"|- EST Limpiando columnas de texto\")\n    logging.debug(\" |- Removiendo espacios en blanco y caracteres especiales\")",
        "detail": "process.clean_transform.dim_establecimiento",
        "documentation": {}
    },
    {
        "label": "limpiar_columas_texto",
        "kind": 2,
        "importPath": "process.clean_transform.dim_establecimiento",
        "description": "process.clean_transform.dim_establecimiento",
        "peekOfCode": "def limpiar_columas_texto(df: pl.DataFrame):\n    logging.info(\"|- EST Limpiando columnas de texto\")\n    logging.debug(\" |- Removiendo espacios en blanco y caracteres especiales\")\n    df = df.with_columns(\n        [pl.col(col).str.strip_chars().str.replace_all(r\"[^a-zA-Z0-9áéíóúÁÉÍÓÚñÑüÜ\\s]\", \"\") for col in df.columns if df[col].dtype == pl.Utf8]\n    )\n    logging.debug(\" |- Convirtiendo a mayúsculas\")\n    df = df.with_columns(\n        [pl.col(col).str.to_uppercase() for col in df.columns if df[col].dtype == pl.Utf8]\n    )",
        "detail": "process.clean_transform.dim_establecimiento",
        "documentation": {}
    },
    {
        "label": "process_establecimiento_orchester",
        "kind": 2,
        "importPath": "process.clean_transform.dim_establecimiento",
        "description": "process.clean_transform.dim_establecimiento",
        "peekOfCode": "def process_establecimiento_orchester(df: pl.DataFrame):\n    logging.info(\"|- Establecimiento Orchester\")\n    #\n    df = limpiar_columnas_geograficas(df, ['LONGPS', 'LATGPS'])\n    #\n    df = limpiar_columas_texto(df)\n    return df",
        "detail": "process.clean_transform.dim_establecimiento",
        "documentation": {}
    },
    {
        "label": "generar_dim_fecha",
        "kind": 2,
        "importPath": "process.clean_transform.dim_fecha",
        "description": "process.clean_transform.dim_fecha",
        "peekOfCode": "def generar_dim_fecha(df: pl.DataFrame):\n    logging.info(\"|- DIM Fecha\")\n    ## generar fechas desde 2020-01-01 hasta 2025-12-31\n    df = df.with_columns(\n        pl.date_range(\n            start=\"2020-01-01\",\n            end=\"2025-12-31\",\n            closed=\"both\",\n            name=\"FECHA\"\n        )",
        "detail": "process.clean_transform.dim_fecha",
        "documentation": {}
    },
    {
        "label": "persona_orchester",
        "kind": 2,
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "peekOfCode": "def persona_orchester(df: pl.DataFrame):\n    df = _crear_dataframe_con_moda_fecha(df)\n    df = _limpiar_columnas_texto(df, cols=[\"TIPO_IDEN\", \"NUM_IDEN\", \"APELLIDOS\", \"NOMBRES\",\"NOMBRES_COMPLETOS\", \"SEXO\", \"ETNIA\", \"NACIONALIDAD\"])\n    df = _limpiar_columnas_fecha(df, cols=[\"FECHA_NACIMIENTO\"])\n    df = _limpiar_identificacion(df)\n    df = _calcular_edad(df)\n    df = _calcular_grupo_etario(df)\n    df = _homologar_etnia(df)\n    return df",
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "peekOfCode": "__all__ = [\n    'persona_orchester',\n]\ndef _limpiar_columnas_texto(df: pl.DataFrame, cols: list[str] = []):\n    logging.info(\"|- EST Limpiando columnas de texto\")\n    for col in cols:\n        logging.debug(f\" |- Limpiando columna {col} caracteres especiales\")\n        df = df.with_columns(pl.col(col).str.strip_chars().alias(col))\n        logging.debug(f\" |- Limpiando columna {col} mayusculas\")\n        df = df.with_columns(pl.col(col).str.to_uppercase().alias(col))",
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "vacunas_nombres",
        "kind": 2,
        "importPath": "process.clean_transform.dim_vacuna",
        "description": "process.clean_transform.dim_vacuna",
        "peekOfCode": "def vacunas_nombres(df):\n    ## leer archivo csv \n    pass\ndef vacuna_orchester(df):\n    return df",
        "detail": "process.clean_transform.dim_vacuna",
        "documentation": {}
    },
    {
        "label": "vacuna_orchester",
        "kind": 2,
        "importPath": "process.clean_transform.dim_vacuna",
        "description": "process.clean_transform.dim_vacuna",
        "peekOfCode": "def vacuna_orchester(df):\n    return df",
        "detail": "process.clean_transform.dim_vacuna",
        "documentation": {}
    },
    {
        "label": "vacunacion_orchester",
        "kind": 2,
        "importPath": "process.clean_transform.dim_vacunacion",
        "description": "process.clean_transform.dim_vacunacion",
        "peekOfCode": "def vacunacion_orchester(df: polars.DataFrame):\n    return df",
        "detail": "process.clean_transform.dim_vacunacion",
        "documentation": {}
    },
    {
        "label": "process_orchester",
        "kind": 2,
        "importPath": "process.clean_transform_orchester",
        "description": "process.clean_transform_orchester",
        "peekOfCode": "def process_orchester():\n    '''\n    Orquesta el procesamiento de datos de vacunación\n    '''\n    df = load_data()\n    df = persona_orchester(df)\n    df = vacuna_orchester(df)\n    df = vacunacion_orchester(df)\n    return df",
        "detail": "process.clean_transform_orchester",
        "documentation": {}
    },
    {
        "label": "date_transform",
        "kind": 2,
        "importPath": "utils.clean.date_transform",
        "description": "utils.clean.date_transform",
        "peekOfCode": "def date_transform(df: DataFrame, column:str, format:str)->DataFrame:\n    df = df.with_columns(\n        polars.col(column).str.strptime(polars.Date,\n                                        fmt=format).alias(column)\n    )\n    return df",
        "detail": "utils.clean.date_transform",
        "documentation": {}
    },
    {
        "label": "remove_extra_whitespaces",
        "kind": 2,
        "importPath": "utils.clean.text_transform",
        "description": "utils.clean.text_transform",
        "peekOfCode": "def remove_extra_whitespaces(df: pl.DataFrame,\n                             columns: list[str]) -> pl.DataFrame:\n    \"\"\"\n    Remove extra whitespaces from the text.\n    Args:\n        df (polars.DataFrame): The input DataFrame.\n    Returns:\n        polars.DataFrame: The cleaned DataFrame.\n    \"\"\"\n    return df.with_columns(",
        "detail": "utils.clean.text_transform",
        "documentation": {}
    },
    {
        "label": "convertir_columnas_a_minusculas",
        "kind": 2,
        "importPath": "utils.df_utils",
        "description": "utils.df_utils",
        "peekOfCode": "def convertir_columnas_a_minusculas(df):\n    \"\"\"\n    Convierte los nombres de las columnas de un DataFrame a minúsculas.\n    Parámetros:\n    df (pandas.DataFrame): El DataFrame cuyas columnas se desean convertir.\n    Retorna:\n    pandas.DataFrame: El DataFrame con los nombres de las columnas en minúsculas.\n    \"\"\"\n    df.columns = [col.lower() for col in df.columns]\n    return df",
        "detail": "utils.df_utils",
        "documentation": {}
    },
    {
        "label": "merge_parquet_files",
        "kind": 2,
        "importPath": "get_full_parquet",
        "description": "get_full_parquet",
        "peekOfCode": "def merge_parquet_files(input_dir: str, output_file: str):\n    spark = SparkSession.builder.getOrCreate()\n    parquet_files = glob.glob(os.path.join(input_dir, \"*.parquet\"))\n    print(f\"Found {len(parquet_files)} parquet files to merge.\")\n    if not parquet_files:\n        print(\"No parquet files found in the directory.\")\n        spark.stop()\n        return\n    df = spark.read.parquet(*parquet_files)\n    df.write.mode(\"overwrite\").parquet(output_file)",
        "detail": "get_full_parquet",
        "documentation": {}
    },
    {
        "label": "fetch_and_save_parquet_oracle",
        "kind": 2,
        "importPath": "get_full_parquet",
        "description": "get_full_parquet",
        "peekOfCode": "def fetch_and_save_parquet_oracle(\n    user: str,\n    password: str,\n    host: str,\n    port: int,\n    service_name: str,\n    table_name: str,\n    parquet_file: str,\n<<<<<<< HEAD\n    chunk_size: int = 5000000",
        "detail": "get_full_parquet",
        "documentation": {}
    },
    {
        "label": "insert_parquets_to_duckdb",
        "kind": 2,
        "importPath": "joinparquet",
        "description": "joinparquet",
        "peekOfCode": "def insert_parquets_to_duckdb(input_dir: str, db_path: str, table_name: str):\n    con = duckdb.connect(database=db_path)\n    con.execute(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM parquet_scan('./parquets/db_covid_19.parquet_chunk3.parquet')\n    \"\"\")\n    con.execute(f\"\"\"\n        DELETE FROM {table_name}\n    \"\"\")\n    for parquet_file in glob.glob(os.path.join(input_dir, \"*.parquet\")):\n        print(f\"Inserting {parquet_file} into {table_name}\")",
        "detail": "joinparquet",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "kind": 2,
        "importPath": "main_full",
        "description": "main_full",
        "peekOfCode": "def setup_logging():\n    \"\"\"Configurar el sistema de logging\"\"\"\n    # Logging en consola y archivo\n    logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n    # Crear un handler para archivo\n    file_handler = logging.FileHandler('app.log')\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n    # Agregar el handler de archivo al logger root\n    logging.getLogger().addHandler(file_handler)",
        "detail": "main_full",
        "documentation": {}
    },
    {
        "label": "parse_arguments",
        "kind": 2,
        "importPath": "main_full",
        "description": "main_full",
        "peekOfCode": "def parse_arguments():\n    \"\"\"Parsear argumentos de línea de comandos\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Procesamiento completo de datos de vacunación\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\n        '--since', \n        type=str, \n        default='1800-01-01',",
        "detail": "main_full",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "main_full",
        "description": "main_full",
        "peekOfCode": "def main():\n    \"\"\"Función principal del procesamiento\"\"\"\n    # Configurar logging\n    setup_logging()\n    # Parsear argumentos\n    args = parse_arguments()\n    logging.info(f\"Iniciando procesamiento con parámetros:\")\n    logging.info(f\"  - since: {args.since}\")\n    logging.info(f\"  - until: {args.until}\")\n    logging.info(f\"  - chunk_size: {args.chunk_size:,}\")",
        "detail": "main_full",
        "documentation": {}
    }
]