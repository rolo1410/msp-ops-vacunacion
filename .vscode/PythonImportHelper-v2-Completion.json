[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "dotenv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dotenv",
        "description": "dotenv",
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "polars",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "polars",
        "description": "polars",
        "detail": "polars",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Engine",
        "importPath": "sqlalchemy.engine",
        "description": "sqlalchemy.engine",
        "isExtraImport": true,
        "detail": "sqlalchemy.engine",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "DB_VACUNACION",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_oracle_engine",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_GEOSALUD",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_oracle_engine",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_MIP",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_oracle_engine",
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "isExtraImport": true,
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_db_vacunaciones",
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "isExtraImport": true,
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_geo_salud_data",
        "importPath": "extract.geo_salud",
        "description": "extract.geo_salud",
        "isExtraImport": true,
        "detail": "extract.geo_salud",
        "documentation": {}
    },
    {
        "label": "get_mpi_data",
        "importPath": "extract.mpi",
        "description": "extract.mpi",
        "isExtraImport": true,
        "detail": "extract.mpi",
        "documentation": {}
    },
    {
        "label": "add_new_elements_to_lake",
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "isExtraImport": true,
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "generare_bi_echema",
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "isExtraImport": true,
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "duckdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "duckdb",
        "description": "duckdb",
        "detail": "duckdb",
        "documentation": {}
    },
    {
        "label": "limpiar_columnas_geograficas",
        "importPath": "process.clean_transform.dim_establecimiento",
        "description": "process.clean_transform.dim_establecimiento",
        "isExtraImport": true,
        "detail": "process.clean_transform.dim_establecimiento",
        "documentation": {}
    },
    {
        "label": "load_data",
        "importPath": "lake.load_lake",
        "description": "lake.load_lake",
        "isExtraImport": true,
        "detail": "lake.load_lake",
        "documentation": {}
    },
    {
        "label": "persona_orchester",
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "isExtraImport": true,
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "vacuna_orchester",
        "importPath": "process.clean_transform.dim_vacuna",
        "description": "process.clean_transform.dim_vacuna",
        "isExtraImport": true,
        "detail": "process.clean_transform.dim_vacuna",
        "documentation": {}
    },
    {
        "label": "vacunacion_orchester",
        "importPath": "process.clean_transform.dim_vacunacion",
        "description": "process.clean_transform.dim_vacunacion",
        "isExtraImport": true,
        "detail": "process.clean_transform.dim_vacunacion",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "ingest_orchester",
        "importPath": "extract.ingest_orchester",
        "description": "extract.ingest_orchester",
        "isExtraImport": true,
        "detail": "extract.ingest_orchester",
        "documentation": {}
    },
    {
        "label": "process_orchester",
        "importPath": "process.clean_transform_orchester",
        "description": "process.clean_transform_orchester",
        "isExtraImport": true,
        "detail": "process.clean_transform_orchester",
        "documentation": {}
    },
    {
        "label": "get_oracle_engine",
        "kind": 2,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "def get_oracle_engine(options: dict) -> Engine:\n    user = options.get(\"user\", \"user\")\n    password = options.get(\"password\", \"password\")\n    host = options.get(\"host\", \"localhost\")\n    port = options.get(\"port\", 1521)\n    service_name = options.get(\"service_name\", \"orclpdb1\")\n    connection_string = f'oracle+oracledb://{user}:{password}@{host}:{port}/?service_name={service_name}'\n    engine: Engine = create_engine(connection_string, pool_pre_ping=True)\n    return engine\ndef postgres_get_engine(options: dict) -> Engine:",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "postgres_get_engine",
        "kind": 2,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "def postgres_get_engine(options: dict) -> Engine:\n    user = options.get(\"user\", \"user\")\n    password = options.get(\"password\", \"password\")\n    host = options.get(\"host\", \"localhost\")\n    port = options.get(\"port\", 5432)\n    dbname = options.get(\"dbname\", \"dbname\")\n    connection_string = f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}'\n    engine: Engine = create_engine(connection_string, pool_pre_ping=True)\n    return engine",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_VACUNACION",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_VACUNACION = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_VACUNACION_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_VACUNACION_SID\", \"orcl\")\n}\nDB_MIP = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_MPI_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_MPI_PORT\", 1521)),",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_MIP",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_MIP = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_MPI_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_MPI_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_MPI_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_MPI_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_MPI_SID\", \"orcl\")\n}\nDB_GEOSALUD = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_GEOSALUD_PORT\", 1521)),",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_GEOSALUD",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_GEOSALUD = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_GEOSALUD_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_GEOSALUD_DB_NAME\", \"geoserver\")\n}\nDB_REPLICA = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_REPLICACION_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_REPLICACION_PORT\", 1521)),",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "DB_REPLICA",
        "kind": 5,
        "importPath": "extract.config.sources",
        "description": "extract.config.sources",
        "peekOfCode": "DB_REPLICA = {\n    \"host\": os.getenv(\"CNN_ORACLE_DB_REPLICACION_HOST\", \"localhost\"),\n    \"port\": int(os.getenv(\"CNN_ORACLE_DB_REPLICACION_PORT\", 1521)),\n    \"user\": os.getenv(\"CNN_ORACLE_DB_REPLICACION_USER\", \"user\"),\n    \"password\": os.getenv(\"CNN_ORACLE_DB_REPLICACION_PASSWORD\", \"password\"),\n    \"service_name\": os.getenv(\"CNN_ORACLE_DB_REPLICACION_DBNAME\", \"replicacion\")\n}\ndef get_oracle_engine(options: dict) -> Engine:\n    user = options.get(\"user\", \"user\")\n    password = options.get(\"password\", \"password\")",
        "detail": "extract.config.sources",
        "documentation": {}
    },
    {
        "label": "get_db_vacunacion",
        "kind": 2,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "def get_db_vacunacion(since, until, offset=0, chunk_size=100000) -> pl.DataFrame:\n    db_vacunacion_engine = get_oracle_engine(DB_VACUNACION)\n    query = f\"\"\"\n            SELECT\n                ID_VAC_DEPU,\n                FECHA_APLICACION,\n                PUNTO_VACUNACION,\n                UNICODIGO,\n                TIPO_IDEN,\n                NUM_IDEN,",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_count_db_vacunacion",
        "kind": 2,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "def get_count_db_vacunacion(since, until):\n    db_vacunacion_engine = get_oracle_engine(DB_VACUNACION)\n    query = f\"\"\"\n            SELECT \n                COUNT(*) AS total_count\n            FROM HCUE_VACUNACION_DEPURADA.DB_VACUNACION_CONSOLIDADA_DEPURADA_COVID\n            WHERE \n                FECHA_APLICACION BETWEEN TO_DATE('{since}', 'YYYY-MM-DD') \n                AND TO_DATE('{until}', 'YYYY-MM-DD')\n            \"\"\"  # Replace with actual query",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_db_vacunaciones",
        "kind": 2,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "def get_db_vacunaciones(since, until, chunk_size=100000) -> pl.DataFrame:\n    logging.info(f\"|- Fetching vacunas\")\n    offset = 0\n    all_data = []\n    total = get_count_db_vacunacion(since, until)\n    for offset in range(0, total, chunk_size):\n        logging.info(f\" |- Fetching records {offset} to {offset + chunk_size} de un total de {total}\")\n        chunk = get_db_vacunacion(since, until, offset, chunk_size)\n        if chunk.is_empty():\n            break",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "VACUNACION_SCHEMA",
        "kind": 5,
        "importPath": "extract.db_vacunacion",
        "description": "extract.db_vacunacion",
        "peekOfCode": "VACUNACION_SCHEMA = {\n    \"ID_VAC_DEPU\": pl.String,\n    \"FECHA_APLICACION\": pl.Date,\n    \"PUNTO_VACUNACION\": pl.String,\n    \"UNICODIGO\": pl.String,\n    \"TIPO_IDEN\": pl.String,\n    \"NUM_IDEN\": pl.String,\n    \"APELLIDOS\": pl.String,\n    \"NOMBRES\": pl.String,\n    \"NOMBRES_COMPLETOS\": pl.String,",
        "detail": "extract.db_vacunacion",
        "documentation": {}
    },
    {
        "label": "get_geo_salud_data",
        "kind": 2,
        "importPath": "extract.geo_salud",
        "description": "extract.geo_salud",
        "peekOfCode": "def get_geo_salud_data():\n    db_geo_salud_data = get_oracle_engine(DB_GEOSALUD)\n    query = f\"\"\"\n            SELECT\n                UNI_CODIGO,\n                UNI_NOMBRE,\n                PRV_CODIGO,\n                PRV_DESCRIPCION,\n                CAN_CODIGO,\n                CAN_DESCRIPCION,",
        "detail": "extract.geo_salud",
        "documentation": {}
    },
    {
        "label": "ingest_orchester",
        "kind": 2,
        "importPath": "extract.ingest_orchester",
        "description": "extract.ingest_orchester",
        "peekOfCode": "def ingest_orchester(since, until):\n    df = get_db_vacunaciones(since, until)\n    ## remover caracter ' especiales de la columna NUM_IDEN y castearla a string\n    df.with_columns(\n        df['NUM_IDEN'].str.replace_all(\"'\", \"\").cast(str)\n    )\n    add_new_elements_to_lake('vacunacion', 'lk_vacunacion', ['NUM_IDEN', 'FECHA_APLICACION', 'UNICODIGO'], df)\n    # datos del registro civil\n    mpi_df = get_mpi_data(df['NUM_IDEN'].drop_nulls().drop_nans().unique().to_list())\n    add_new_elements_to_lake('vacunacion', 'lk_persona', ['IDENTIFIER_VALUE'], mpi_df)",
        "detail": "extract.ingest_orchester",
        "documentation": {}
    },
    {
        "label": "get_mpi_data_chunk",
        "kind": 2,
        "importPath": "extract.mpi",
        "description": "extract.mpi",
        "peekOfCode": "def get_mpi_data_chunk(identifications: list[str]) -> pl.DataFrame:\n    db_mpi_data = get_oracle_engine(DB_MIP)\n    query = f\"\"\"\n            SELECT\n                EC_IDENTIFIER_OID,\n                IDENTIFIER_VALUE,\n                GENDER,\n                BIRTHDATE,\n                MARITAL_STATUS,\n                EC_FAMILY_GROUP,",
        "detail": "extract.mpi",
        "documentation": {}
    },
    {
        "label": "get_mpi_data",
        "kind": 2,
        "importPath": "extract.mpi",
        "description": "extract.mpi",
        "peekOfCode": "def get_mpi_data(identifications: list[str]) -> pl.DataFrame:\n    logging.info(f\"|- MPI Obteniendo datos del MPI para {len(identifications)} identificaciones\")\n    chunk_size = 999\n    dfs = []\n    # por configuracion solo se puede traer en chunks de 999\n    for i in range(0, len(identifications), chunk_size):\n        logging.info(f\" |- Fetching chunk {i // chunk_size + 1} de {(len(identifications) - 1) // chunk_size + 1}\")\n        chunk = identifications[i:i + chunk_size]\n        df = get_mpi_data_chunk(chunk)\n        dfs.append(df)",
        "detail": "extract.mpi",
        "documentation": {}
    },
    {
        "label": "create_oracle_engine",
        "kind": 2,
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "peekOfCode": "def create_oracle_engine(config, type):\n    try:\n        engine = create_engine(\"postgresql+psycopg2://scott:tiger@localhost:5432/mydatabase\")\n        return engine\n    except Exception as e:\n        logging.info(f\"Error al establecer la conexión con la base de datos: {e}\")\ndef save_to_lake(data_chunk):\n    # Implement the logic to save the data chunk to the lake\n    logging.info(f\"Saving chunk of size {len(data_chunk)} to the lake...\")\n    pass    ",
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "save_to_lake",
        "kind": 2,
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "peekOfCode": "def save_to_lake(data_chunk):\n    # Implement the logic to save the data chunk to the lake\n    logging.info(f\"Saving chunk of size {len(data_chunk)} to the lake...\")\n    pass    \ndef process_data(engine, since, until, chunk_size=10000):\n    try:\n        with engine.connect() as connection:\n            result = connection.execute(\n                \"SELECT * FROM my_table WHERE date >= :since AND date < :until\",\n                {\"since\": since, \"until\": until}",
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "process_data",
        "kind": 2,
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "peekOfCode": "def process_data(engine, since, until, chunk_size=10000):\n    try:\n        with engine.connect() as connection:\n            result = connection.execute(\n                \"SELECT * FROM my_table WHERE date >= :since AND date < :until\",\n                {\"since\": since, \"until\": until}\n            )\n            for chunk in iter(lambda: list(itertools.islice(result, chunk_size)), []):\n                process_chunk(chunk)\n    except Exception as e:",
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "ingest_data_lake",
        "kind": 2,
        "importPath": "lake.ingest_lake",
        "description": "lake.ingest_lake",
        "peekOfCode": "def ingest_data_lake(since, until, chunk_size=10000):\n    logging.basicConfig(level=logging.INFO)\n    logging.info(\"Iniciando el proceso de ingestión de datos al lago...\")\n    engine = create_oracle_engine(\"DB_CONFIG\", \"oracle\")\n    if not engine:\n        logging.error(\"No se pudo crear el motor de la base de datos. Terminando el proceso.\")\n        return\n    process_data(engine, since, until, chunk_size)\n    logging.info(\"Proceso de ingestión de datos al lago completado.\")",
        "detail": "lake.ingest_lake",
        "documentation": {}
    },
    {
        "label": "generate_lake_schema",
        "kind": 2,
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "peekOfCode": "def generate_lake_schema():\n    # Implement the logic to generate the lake schema\n    con = duckdb.connect('./resources/data_lake/vacunacion.duckdb')\n    con.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS lake_schema (\n            id INTEGER PRIMARY KEY,\n            name VARCHAR,\n            value DOUBLE\n        )\n    \"\"\")",
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "generare_bi_echema",
        "kind": 2,
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "peekOfCode": "def generare_bi_echema():\n    # Implement the logic to generate the BI schema\n    con = duckdb.connect('./resources/data_lake/vacunacion.duckdb')\n    con.execute(\"\"\"\n        CREATE TABLE IF NOT EXISTS dim_persona (\n            id INTEGER PRIMARY KEY,\n            nombres VARCHAR),\n            apellidos VARCHAR,\n            fecha_nacimiento DATE,\n            identificacion VARCHAR,",
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "add_new_elements_to_lake",
        "kind": 2,
        "importPath": "lake.init_lake",
        "description": "lake.init_lake",
        "peekOfCode": "def add_new_elements_to_lake(db:str,\n                              table:str,\n                              keys_columns:list[str],\n                              df:pl.DataFrame):\n    logging.info(f\"|-Adding new elements to lake: {db}.{table}\")\n    # Implement the logic to add new elements to the lake\n    con = duckdb.connect(f'./resources/data_lake/{db}.duckdb')\n    # \n    one_query = f\"\"\"CREATE TABLE IF NOT EXISTS {db}.main.{table} AS SELECT * FROM df;\n                    CREATE TABLE IF NOT EXISTS {db}.main.tmp_{table} AS SELECT * FROM df;",
        "detail": "lake.init_lake",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "lake.load_lake",
        "description": "lake.load_lake",
        "peekOfCode": "def load_data()-> pl.DataFrame:\n    # Implement the logic to load data into the lake\n    logging.info(\"|- Cargando datos al lago\")\n    con = duckdb.connect(database='./resources/data_lake/vacunacion.duckdb')\n    df = con.execute(\"SELECT * FROM lk_vacunacion\").fetch_df()\n    df = pl.from_pandas(df)\n    logging.info(\". |- Datos cargados al lago\")\n    return df",
        "detail": "lake.load_lake",
        "documentation": {}
    },
    {
        "label": "DATA_SOURCES",
        "kind": 5,
        "importPath": "lake.sources",
        "description": "lake.sources",
        "peekOfCode": "DATA_SOURCES = [\n    {\n        \"name\": \"MPI\",\n        \"type\": \"postgresql\",\n        \"host\": \"localhost\",\n        \"port\": 5432,\n        \"database\": \"mydatabase\",\n        \"username\": \"scott\",\n        \"password\": \"tiger\"              \n    }, {",
        "detail": "lake.sources",
        "documentation": {}
    },
    {
        "label": "limpiar_columnas_geograficas",
        "kind": 2,
        "importPath": "process.clean_transform.dim_establecimiento",
        "description": "process.clean_transform.dim_establecimiento",
        "peekOfCode": "def limpiar_columnas_geograficas(df: pl.DataFrame, cols: list):\n    logging.info(\"|- EST \")\n    logging.debug(\" |- Truncando latitud y longitud a 6 decimales\")\n    df = df.with_columns(\n        [pl.col(col).round(6) for col in cols if col in df.columns]\n    )  \n    return df\ndef limpiar_columas_texto(df: pl.DataFrame):\n    logging.info(\"|- EST Limpiando columnas de texto\")\n    logging.debug(\" |- Removiendo espacios en blanco y caracteres especiales\")",
        "detail": "process.clean_transform.dim_establecimiento",
        "documentation": {}
    },
    {
        "label": "limpiar_columas_texto",
        "kind": 2,
        "importPath": "process.clean_transform.dim_establecimiento",
        "description": "process.clean_transform.dim_establecimiento",
        "peekOfCode": "def limpiar_columas_texto(df: pl.DataFrame):\n    logging.info(\"|- EST Limpiando columnas de texto\")\n    logging.debug(\" |- Removiendo espacios en blanco y caracteres especiales\")\n    df = df.with_columns(\n        [pl.col(col).str.strip_chars().str.replace_all(r\"[^a-zA-Z0-9áéíóúÁÉÍÓÚñÑüÜ\\s]\", \"\") for col in df.columns if df[col].dtype == pl.Utf8]\n    )\n    logging.debug(\" |- Convirtiendo a mayúsculas\")\n    df = df.with_columns(\n        [pl.col(col).str.to_uppercase() for col in df.columns if df[col].dtype == pl.Utf8]\n    )",
        "detail": "process.clean_transform.dim_establecimiento",
        "documentation": {}
    },
    {
        "label": "process_establecimiento_orchester",
        "kind": 2,
        "importPath": "process.clean_transform.dim_establecimiento",
        "description": "process.clean_transform.dim_establecimiento",
        "peekOfCode": "def process_establecimiento_orchester(df: pl.DataFrame):\n    logging.info(\"|- Establecimiento Orchester\")\n    #\n    df = limpiar_columnas_geograficas(df, ['LONGPS', 'LATGPS'])\n    #\n    df = limpiar_columas_texto(df)\n    return df",
        "detail": "process.clean_transform.dim_establecimiento",
        "documentation": {}
    },
    {
        "label": "generar_dim_fecha",
        "kind": 2,
        "importPath": "process.clean_transform.dim_fecha",
        "description": "process.clean_transform.dim_fecha",
        "peekOfCode": "def generar_dim_fecha(df: pl.DataFrame):\n    logging.info(\"|- DIM Fecha\")\n    ## generar fechas desde 2020-01-01 hasta 2025-12-31\n    df = df.with_columns(\n        pl.date_range(\n            start=\"2020-01-01\",\n            end=\"2025-12-31\",\n            closed=\"both\",\n            name=\"FECHA\"\n        )",
        "detail": "process.clean_transform.dim_fecha",
        "documentation": {}
    },
    {
        "label": "limpiar_columnas_texto",
        "kind": 2,
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "peekOfCode": "def limpiar_columnas_texto(df: pl.DataFrame, cols: list[str] = []):\n    logging.info(\"|- EST Limpiando columnas de texto\")\n    for col in cols:\n        logging.debug(f\"  |- Limpiando columna {col}\")\n        df = df.with_columns(pl.col(col).str.strip_chars().alias(col))\n    return df\ndef limpiar_columnas_fecha(df: pl.DataFrame, cols: list[str] = []):\n    logging.info(\"|- EST Estandarizando columnas fecha\")\n    for col in cols:\n        logging.info(f\"  |- Estandarizando columna {col}\")",
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "limpiar_columnas_fecha",
        "kind": 2,
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "peekOfCode": "def limpiar_columnas_fecha(df: pl.DataFrame, cols: list[str] = []):\n    logging.info(\"|- EST Estandarizando columnas fecha\")\n    for col in cols:\n        logging.info(f\"  |- Estandarizando columna {col}\")\n    return df\ndef esCedulaValida(cedula: str) -> bool:    \n    # Implementación de la validación de cédula\n    if not cedula or len(cedula) != 10 or not cedula.isdigit():\n        return False\n    coeficientes = [2, 1, 2, 1, 2, 1, 2, 1, 2]",
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "esCedulaValida",
        "kind": 2,
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "peekOfCode": "def esCedulaValida(cedula: str) -> bool:    \n    # Implementación de la validación de cédula\n    if not cedula or len(cedula) != 10 or not cedula.isdigit():\n        return False\n    coeficientes = [2, 1, 2, 1, 2, 1, 2, 1, 2]\n    total = 0\n    for i in range(9):\n        val = int(cedula[i]) * coeficientes[i]\n        if val >= 10:\n            val -= 9",
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "limpiar_identificacion",
        "kind": 2,
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "peekOfCode": "def limpiar_identificacion(df: pl.DataFrame):\n    logging.info(\"|- LIM Limpiando columnas identificación\")\n    ## Eliminar registros sin cédula\n    logging.debug(f\"  |- REM Eliminando registros sin cédula o vacíos en el campo NUM_IDEN\")\n    df = df.filter(pl.col(\"NUM_IDEN\").is_not_null() & (pl.col(\"NUM_IDEN\") != \"\"))\n    ## si el registro es cedula y tiene 10 digitos\n    logging.debug(f\"  |- EST Completando cedulas que tienen menos de 10 digitos con un 0 a la izquierda\")\n    df = df.with_columns(pl.when(\n        (pl.col(\"TIPO_IDEN\") == \"CÉDULA DE IDENTIDAD\") & (pl.col(\"NUM_IDEN\").str.len_chars() < 10)\n    ).then(",
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "clean_anios_1900",
        "kind": 2,
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "peekOfCode": "def clean_anios_1900(df: pl.DataFrame):\n    logging.info(\"|- VAL Estandarizando columnas cédulas\")\n    logging.debug(f\"  |- No implementadoEstandarizando columnas cédulas\")\n    return df\ndef calcular_edad(df: pl.DataFrame):\n    logging.info(\"|- ENR Agregando edad, descompiendo en años, meses y días\")\n    df['edad_anios'] = df.apply(calcular_edad)\n    df['edad_meses'] = df.apply(calcular_edad)\n    df['edad_dias'] = df.apply(calcular_edad)\n    return df",
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "calcular_edad",
        "kind": 2,
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "peekOfCode": "def calcular_edad(df: pl.DataFrame):\n    logging.info(\"|- ENR Agregando edad, descompiendo en años, meses y días\")\n    df['edad_anios'] = df.apply(calcular_edad)\n    df['edad_meses'] = df.apply(calcular_edad)\n    df['edad_dias'] = df.apply(calcular_edad)\n    return df\ndef calcular_grupo_etario(df: pl.DataFrame):\n    # agrega la columna grupo_etario\n    logging.info(\"|- ENR Agregando edad\")\n    df['grupo_etario'] = df.apply(calcular_edad)    ",
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "calcular_grupo_etario",
        "kind": 2,
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "peekOfCode": "def calcular_grupo_etario(df: pl.DataFrame):\n    # agrega la columna grupo_etario\n    logging.info(\"|- ENR Agregando edad\")\n    df['grupo_etario'] = df.apply(calcular_edad)    \n    logging.debug(\". |- No implementadoAgregando edad\")\n    logging.info(\"|- ENR Desagregando edad en días , mes , año\")\n    logging.debug(\". |- No implementadoDesagregando edad en días , mes , año\")\n    return df\ndef persona_orchester(df: pl.DataFrame):\n    df = limpiar_columnas_texto(df, cols=[\"TIPO_IDEN\", \"NUM_IDEN\", \"APELLIDOS\", \"NOMBRES\",\"NOMBRES_COMPLETOS\", \"SEXO\", \"ETNIA\", \"NACIONALIDAD\"])",
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "persona_orchester",
        "kind": 2,
        "importPath": "process.clean_transform.dim_persona",
        "description": "process.clean_transform.dim_persona",
        "peekOfCode": "def persona_orchester(df: pl.DataFrame):\n    df = limpiar_columnas_texto(df, cols=[\"TIPO_IDEN\", \"NUM_IDEN\", \"APELLIDOS\", \"NOMBRES\",\"NOMBRES_COMPLETOS\", \"SEXO\", \"ETNIA\", \"NACIONALIDAD\"])\n    df = limpiar_columnas_fecha(df, cols=[\"FECHA_NACIMIENTO\"])\n    df = limpiar_identificacion(df)\n    df = validar_cedulas(df)\n    df = clean_anios_1900(df)\n    df = calcular_edad(df)\n    df = calcular_grupo_etario(df)\n    df = limpiar_identificacion(df)\n    return df",
        "detail": "process.clean_transform.dim_persona",
        "documentation": {}
    },
    {
        "label": "vacuna_orchester",
        "kind": 2,
        "importPath": "process.clean_transform.dim_vacuna",
        "description": "process.clean_transform.dim_vacuna",
        "peekOfCode": "def vacuna_orchester(df):\n    return df",
        "detail": "process.clean_transform.dim_vacuna",
        "documentation": {}
    },
    {
        "label": "vacunacion_orchester",
        "kind": 2,
        "importPath": "process.clean_transform.dim_vacunacion",
        "description": "process.clean_transform.dim_vacunacion",
        "peekOfCode": "def vacunacion_orchester(df: polars.DataFrame):\n    return df",
        "detail": "process.clean_transform.dim_vacunacion",
        "documentation": {}
    },
    {
        "label": "process_orchester",
        "kind": 2,
        "importPath": "process.clean_transform_orchester",
        "description": "process.clean_transform_orchester",
        "peekOfCode": "def process_orchester():\n    df = load_data()\n    df = persona_orchester(df)\n    df = vacuna_orchester(df)\n    df = vacunacion_orchester(df)\n    return df",
        "detail": "process.clean_transform_orchester",
        "documentation": {}
    },
    {
        "label": "date_transform",
        "kind": 2,
        "importPath": "utils.clean.date_transform",
        "description": "utils.clean.date_transform",
        "peekOfCode": "def date_transform(df: DataFrame, column:str, format:str)->DataFrame:\n    df = df.with_columns(\n        polars.col(column).str.strptime(polars.Date,\n                                        fmt=format).alias(column)\n    )\n    return df",
        "detail": "utils.clean.date_transform",
        "documentation": {}
    },
    {
        "label": "remove_extra_whitespaces",
        "kind": 2,
        "importPath": "utils.clean.text_transform",
        "description": "utils.clean.text_transform",
        "peekOfCode": "def remove_extra_whitespaces(df: pl.DataFrame,\n                             columns: list[str]) -> pl.DataFrame:\n    \"\"\"\n    Remove extra whitespaces from the text.\n    Args:\n        df (polars.DataFrame): The input DataFrame.\n    Returns:\n        polars.DataFrame: The cleaned DataFrame.\n    \"\"\"\n    return df.with_columns(",
        "detail": "utils.clean.text_transform",
        "documentation": {}
    },
    {
        "label": "convertir_columnas_a_minusculas",
        "kind": 2,
        "importPath": "utils.df_utils",
        "description": "utils.df_utils",
        "peekOfCode": "def convertir_columnas_a_minusculas(df):\n    \"\"\"\n    Convierte los nombres de las columnas de un DataFrame a minúsculas.\n    Parámetros:\n    df (pandas.DataFrame): El DataFrame cuyas columnas se desean convertir.\n    Retorna:\n    pandas.DataFrame: El DataFrame con los nombres de las columnas en minúsculas.\n    \"\"\"\n    df.columns = [col.lower() for col in df.columns]\n    return df",
        "detail": "utils.df_utils",
        "documentation": {}
    },
    {
        "label": "file_handler",
        "kind": 5,
        "importPath": "main_full",
        "description": "main_full",
        "peekOfCode": "file_handler = logging.FileHandler('app.log')\nfile_handler.setLevel(logging.INFO)\nfile_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n# Agregar el handler de archivo al logger root\nlogging.getLogger().addHandler(file_handler)\n##\nif __name__ == \"__main__\":\n    since = \"2023-03-01\"\n    until = \"2023-03-06\" \n    # ingest_orchester(since, until)",
        "detail": "main_full",
        "documentation": {}
    }
]